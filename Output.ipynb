{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "import * only allowed at module level (<ipython-input-1-816e9ae2589f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-816e9ae2589f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from __future__ import print_function\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m import * only allowed at module level\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from __future__ import print_function\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "import warnings\n",
    "import nltk \n",
    "import re\n",
    "import random\n",
    "from datetime import timedelta, date\n",
    "from pandas import DataFrame\n",
    "from nltk import *\n",
    "from nltk.corpus import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "amc_model = load_model('AMCModel.h5')\n",
    "gme_model = load_model('GMEModel.h5')\n",
    "\n",
    "amc_testing = pd.read_csv(r'./Data/AMC_testing_data.csv', index_col=0, parse_dates=True)\n",
    "gme_testing = pd.read_csv(r'./Data/GME_testing_data.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Stock Prediction\n",
    "\n",
    "\n",
    "def Stock():\n",
    "\n",
    "    def split_sequence(sequence, n_steps):\n",
    "        X, y = list(), list()\n",
    "        for i in range(len(sequence)):\n",
    "            # find the end of this pattern\n",
    "            end_ix = i + n_steps\n",
    "            # check if we are beyond the sequence\n",
    "            if end_ix > len(sequence)-1:\n",
    "                break\n",
    "            # gather input and output parts of the pattern\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return array(X), array(y)\n",
    "\n",
    "\n",
    "\n",
    "def testing_data_prep(testing_set, model):\n",
    "    n_steps = 8\n",
    "    #data here taken from csv columns, have a look at the tetsing set for the format\n",
    "    price_testing = testing_set['Price'].values\n",
    "    price_open_testing = testing_set['Open'].values\n",
    "    price_high_testing = testing_set['High'].values\n",
    "    price_low_testing = testing_set['Low'].values\n",
    "    price_vol_testing = testing_set['Vol.'].values\n",
    "    price_change_testing = testing_set['Change %'].values\n",
    "\n",
    "    #runs the above split function to make the datset into small arrays with 8 steps each, copy the function for prediction\n",
    "    #this will need to be done to any prediction, it needs 8 steps behind to predict\n",
    "    Tprice, y = split_sequence(price_testing, n_steps)\n",
    "    Tprice = Tprice.reshape((Tprice.shape[0], n_steps, 1))\n",
    "    \n",
    "    Topen, y = split_sequence(price_open_testing, n_steps)\n",
    "    Topen = Topen.reshape((Topen.shape[0], n_steps, 1))\n",
    "    \n",
    "    Thigh, y = split_sequence(price_high_testing, n_steps)\n",
    "    Thigh = Thigh.reshape((Thigh.shape[0], n_steps, 1))\n",
    "    \n",
    "    Tlow, y = split_sequence(price_low_testing, n_steps)\n",
    "    Tlow = Tlow.reshape((Tlow.shape[0], n_steps, 1))\n",
    "    \n",
    "    TVol, y = split_sequence(price_low_testing, n_steps)\n",
    "    TVol = Tlow.reshape((Tlow.shape[0], n_steps, 1))\n",
    "    \n",
    "    Tchange, y = split_sequence(price_change_testing, n_steps)\n",
    "    Tchange = Tchange.reshape((Tchange.shape[0], n_steps, 1))\n",
    "\n",
    "    #This line takes all of the split arrays and puts them together, shapinng the data structure to fit the lstm\n",
    "    Ttraining = np.concatenate((Tprice, Topen, Thigh, Tlow, Tchange), axis = 2)\n",
    "\n",
    "    #may have to run this line once to prune the end of the data\n",
    "    #this line is just for accuracy testing, it aligns the testing dataset with the first output\n",
    "    price_change_testing_test = price_change_testing[8:]\n",
    "    \n",
    "    #predicts using the correct dataset\n",
    "    predictions = model.predict(Ttraining)\n",
    "    mse = mean_squared_error(price_change_testing_test, predictions)\n",
    "    print(mse)\n",
    "    #with the predictions you can model them or if its just one output it\n",
    "    # Add title and axis names\n",
    "    plt.title('Stock Prediction')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price')\n",
    "    plt.plot(predictions)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title('Stock Prediction')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price')\n",
    "    plt.plot(price_change_testing_test)\n",
    "    plt.savefig('prediction.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    lstm_acc_df = pd.DataFrame()\n",
    "    price_change_testing_test = price_change_testing_test.reshape(price_change_testing_test.shape[0],1)\n",
    "    lstm_acc_df['Actual'] = price_change_testing_test[:,0]\n",
    "    lstm_acc_df['Predict'] = predictions[:,0]\n",
    "    lstm_acc_df.plot()   \n",
    "    \n",
    "    \n",
    "    axes = plt.gca()\n",
    "    y_min, y_max = axes.get_ylim()\n",
    "    #print(y_max) # Print the maximum y value\n",
    "    max_x = 'Days'[y_max.argmax()]  # Find the x value corresponding to the maximum y value\n",
    "    print(\"The stock will reach the highest price of: \")\n",
    "    print(y_max)\n",
    "    print(\"after this amount of days: \")\n",
    "    four = (ord(max_x))\n",
    "    x = four/4\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    StockDate = datetime.datetime.today() + datetime.timedelta(days=x)\n",
    "    \n",
    "    \n",
    "    return StockDate\n",
    "    \n",
    "#runs the testing function, with some modifications and the correct dataset you will be able to fun a similar function\n",
    "testing_data_prep(amc_testing, amc_model)\n",
    "testing_data_prep(gme_testing, gme_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sentiment analysis\n",
    "\n",
    "\n",
    "\n",
    "def sentiment():\n",
    "#Create the stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "    #initialise the training sets from NLTK\n",
    "    positiveNLTK = twitter_samples.strings('positive_tweets.json')\n",
    "    negativeNLTK = twitter_samples.strings('negative_tweets.json')\n",
    "    #set the training sets into a dataframe\n",
    "    posDF = DataFrame(positiveNLTK,columns=['text'])\n",
    "    negDF = DataFrame(negativeNLTK,columns=['text'])\n",
    "\n",
    "    #Read the file \n",
    "    data = pd.read_csv(\"Data/reddit_wsb.csv\") \n",
    "\n",
    "\n",
    "def dataCleaning(data):\n",
    "        #Loop through the data, creating a new dataframe with only ascii characters, removing punctuation etc. \n",
    "        data['text'] = data['text'].apply(lambda s: \"\".join(char for char in s if char.isascii()))\n",
    "        #Removes any words which start with @, which are replies. \n",
    "        data['text']= data['text'].str.replace('(@\\w+.*?)',\"\")\n",
    "        #Remove links, or tokens starting with http \n",
    "        data['text'] = data['text'].str.replace('http[^\\s]*',\"\")\n",
    "        #Remove any left over characters \n",
    "        data = data['text'].str.replace('[^\\w\\s]','')\n",
    "        #return the cleaned data\n",
    "        return data\n",
    "\n",
    "    #Function for removing stopwords in given data\n",
    "def removeStopwords(data):\n",
    "        #Choose the english stopwords\n",
    "        eng_stopwords = stopwords.words('english')\n",
    "        #Set the english stopwords\n",
    "        eng_stopwords = set(eng_stopwords)\n",
    "        #Create a new list/series containing only words which arent in eng_stopwords\n",
    "        result = data.apply(lambda words: [word for word in words if word.lower() not in eng_stopwords])\n",
    "        #The stemming process is built into the remove stopwords function to cut down on individual functions. \n",
    "        #This uses the PorterStemmer, built into NLTK, to stem words. \n",
    "        result = result.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "        return result\n",
    "\n",
    "def wordCount(data):\n",
    "        for words in data:\n",
    "            for word in words:\n",
    "                yield word\n",
    "\n",
    "    #Used for modelling the tweets and posts to the correct format so as they can be fed into the Naive Bayes classifier\n",
    "def modelTweets(data):\n",
    "        for words in data:\n",
    "             yield dict([word, True] for word in words)\n",
    "\n",
    "def toDict(word):\n",
    "    return {word : True}\n",
    "\n",
    "\n",
    "\n",
    "    # droppeddata will store the new dataframe with the dropped columns\n",
    "\n",
    "    #For now, two variables exist, one with the timestamp attached and one without, timestamp will be used in future versions\n",
    "    dataTime = data.drop(columns=['score', 'id','url','comms_num','created','body'])\n",
    "    data = data.drop(columns=['score', 'id','url','comms_num','created','body','timestamp'])\n",
    "\n",
    "    data.columns = ['text']\n",
    "    # drop all rows with NaN and store in alldata\n",
    "    dataTime['Date']=pd.to_datetime(dataTime['timestamp'])\n",
    "    dataTime = dataTime.drop(columns=['timestamp'])\n",
    "    dataTime['text'] = dataTime['title']\n",
    "    dataTime = dataTime.drop(columns=['title'])\n",
    "\n",
    "    #runs the neccesary functions which will clean the data, tokenize it and remove any stopwords.\n",
    "    dataCleaned = dataCleaning(data)\n",
    "    dataCleaned = dataCleaned.apply(word_tokenize)\n",
    "    dataCleaned = removeStopwords(dataCleaned)\n",
    "\n",
    "    #Cleans the positive training set\n",
    "    posDF = dataCleaning(posDF)\n",
    "    posDF = posDF.apply(word_tokenize)\n",
    "    posDF = removeStopwords(posDF)\n",
    "\n",
    "    #Cleans the negative training set\n",
    "    negDF = dataCleaning(negDF)\n",
    "    negDF = negDF.apply(word_tokenize)\n",
    "    negDF = removeStopwords(negDF)\n",
    "\n",
    "    dataCleaned = dataCleaned.tolist()\n",
    "    dataCount = wordCount(dataCleaned)\n",
    "    dataFreq = FreqDist(dataCount)\n",
    "\n",
    "    negList = negDF.tolist()\n",
    "    negWords = wordCount(negList)\n",
    "    negFrq = FreqDist(negWords)\n",
    "\n",
    "    posList = posDF.tolist()\n",
    "    posWords = wordCount(posList)\n",
    "    posFrq = FreqDist(posWords)\n",
    "\n",
    "    posModel = modelTweets(posList)\n",
    "    negModel = modelTweets(negList)\n",
    "    testModel = modelTweets(dataCleaned)\n",
    "\n",
    "    posDataset = [(tweet_dict, 1)\n",
    "                         for tweet_dict in posModel]\n",
    "\n",
    "    negDataset = [(tweet_dict, 0)\n",
    "                         for tweet_dict in negModel]\n",
    "\n",
    "    trainingDataset = posDataset + negDataset\n",
    "    random.shuffle(trainingDataset)\n",
    "    trainData, testData = trainingDataset[7000:], trainingDataset[:7000]\n",
    "    classifier = NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "    print(\"Accuracy :\", classify.accuracy(classifier, testData))\n",
    "\n",
    "    Sentiments = []\n",
    "    for x in testModel:\n",
    "        Sentiments.append(classifier.classify(x))\n",
    "\n",
    "    #Put the sentiment score in the dataframe\n",
    "    scoresdf = pd.DataFrame({'Sentiment': Sentiments})\n",
    "    dataTime['Sentiment'] = scoresdf['Sentiment'].values\n",
    "\n",
    "    positive = Sentiments.count(1)\n",
    "    negative = Sentiments.count(0)\n",
    "\n",
    "    print(\"Number of positive posts = \", positive)\n",
    "    print(\"Number of negative posts = \", negative)\n",
    "    allposts = negative + positive\n",
    "    avgneg = negative / allposts\n",
    "    avgpos = positive / allposts\n",
    "\n",
    "    print(\"Avg positive = \", avgpos)\n",
    "    print(\"Avg negative = \", avgneg)\n",
    "    classifier.show_most_informative_features()\n",
    "\n",
    "    #Removes Hours, Minutes and seconds from dataframe\n",
    "    dataTime['Date'] = pd.to_datetime(dataTime['Date']).dt.date\n",
    "    #Groups entries by date and calculates the mean value of the sentiment scores\n",
    "    avgScore = dataTime.groupby('Date')['Sentiment'].mean()\n",
    "    print(avgScore)\n",
    "    return avgScore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Stock()\n",
    "\n",
    "\n",
    "\n",
    "sentiment()\n",
    "\n",
    "highest_sentiment = max(avgScore)\n",
    "\n",
    "print(\"The highest sentiment score is \", highest_sentiment)\n",
    "if highest_sentiment > 0.5:\n",
    "    print(\"The sentiment score is very high. This indicates stock prices will go up. You Should buy stocks soon.\")\n",
    "    print(\"The stock price will be highest on this day \", StockDate, \". If you buy before this day and sell on the specified day, you will make money!!!\")\n",
    "\n",
    "elif highest_sentiment < 0.5:\n",
    "    print(\"The sentiment score is very low which could indicate the stock prices will go down. We highly recommend you do not buy stocks soon.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
