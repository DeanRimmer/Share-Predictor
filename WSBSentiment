import datetime
import warnings
import nltk
import numpy as np
import pandas as pd 
import re
import random
from datetime import timedelta, date
from pandas import DataFrame
from nltk import *
from nltk.corpus import *
from nltk.tokenize import *
from nltk.stem.porter import *
nltk.download('punkt')
nltk.download('stopwords')

warnings.filterwarnings('ignore')

#Create the stemmer object
stemmer = PorterStemmer()
#initialise the training sets from NLTK
positiveNLTK = twitter_samples.strings('positive_tweets.json')
negativeNLTK = twitter_samples.strings('negative_tweets.json')
#set the training sets into a dataframe
posDF = DataFrame(positiveNLTK,columns=['text'])
negDF = DataFrame(negativeNLTK,columns=['text'])

#Read the file 
data = pd.read_csv("reddit_wsb.csv") 


def dataCleaning(data):
    #Loop through the data, creating a new dataframe with only ascii characters, removing punctuation etc. 
    data['text'] = data['text'].apply(lambda s: "".join(char for char in s if char.isascii()))
    #Removes any words which start with @, which are replies. 
    data['text']= data['text'].str.replace('(@\w+.*?)',"")
    #Remove links, or tokens starting with http 
    data['text'] = data['text'].str.replace('http[^\s]*',"")
    #Remove any left over characters 
    data = data['text'].str.replace('[^\w\s]','')
    #return the cleaned data
    return data

#Function for removing stopwords in given data
def removeStopwords(data):
    #Choose the english stopwords
    eng_stopwords = stopwords.words('english')
    #Set the english stopwords
    eng_stopwords = set(eng_stopwords)
    #Create a new list/series containing only words which arent in eng_stopwords
    result = data.apply(lambda words: [word for word in words if word.lower() not in eng_stopwords])
    #The stemming process is built into the remove stopwords function to cut down on individual functions. 
    #This uses the PorterStemmer, built into NLTK, to stem words. 
    result = result.apply(lambda x: [stemmer.stem(y) for y in x])
    return result

def wordCount(data):
    for words in data:
        for word in words:
            yield word

#Used for modelling the tweets and posts to the correct format so as they can be fed into the Naive Bayes classifier
def modelTweets(data):
    for words in data:
         yield dict([word, True] for word in words)
            
def toDict(word):
     return {word : True}



# droppeddata will store the new dataframe with the dropped columns

#For now, two variables exist, one with the timestamp attached and one without, timestamp will be used in future versions
dataTime = data.drop(columns=['score', 'id','url','comms_num','created','body'])
data = data.drop(columns=['score', 'id','url','comms_num','created','body','timestamp'])

data.columns = ['text']
# drop all rows with NaN and store in alldata
dataTime['Date']=pd.to_datetime(dataTime['timestamp'])
dataTime = dataTime.drop(columns=['timestamp'])
dataTime['text'] = dataTime['title']
dataTime = dataTime.drop(columns=['title'])

#runs the neccesary functions which will clean the data, tokenize it and remove any stopwords.
dataCleaned = dataCleaning(data)
dataCleaned = dataCleaned.apply(word_tokenize)
dataCleaned = removeStopwords(dataCleaned)

#Cleans the positive training set
posDF = dataCleaning(posDF)
posDF = posDF.apply(word_tokenize)
posDF = removeStopwords(posDF)

#Cleans the negative training set
negDF = dataCleaning(negDF)
negDF = negDF.apply(word_tokenize)
negDF = removeStopwords(negDF)

dataCleaned = dataCleaned.tolist()
dataCount = wordCount(dataCleaned)
dataFreq = FreqDist(dataCount)

negList = negDF.tolist()
negWords = wordCount(negList)
negFrq = FreqDist(negWords)

posList = posDF.tolist()
posWords = wordCount(posList)
posFrq = FreqDist(posWords)

posModel = modelTweets(posList)
negModel = modelTweets(negList)
testModel = modelTweets(dataCleaned)

posDataset = [(tweet_dict, 1)
                     for tweet_dict in posModel]

negDataset = [(tweet_dict, 0)
                     for tweet_dict in negModel]

trainingDataset = posDataset + negDataset
random.shuffle(trainingDataset)
trainData, testData = trainingDataset[7000:], trainingDataset[:7000]
classifier = NaiveBayesClassifier.train(trainData)

print("Accuracy :", classify.accuracy(classifier, testData))

Sentiments = []
for x in testModel:
    Sentiments.append(classifier.classify(x))

#Put the sentiment score in the dataframe
scoresdf = pd.DataFrame({'Sentiment': Sentiments})
dataTime['Sentiment'] = scoresdf['Sentiment'].values

positive = Sentiments.count(1)
negative = Sentiments.count(0)

print("Number of positive posts = ", positive)
print("Number of negative posts = ", negative)
allposts = negative + positive
avgneg = negative / allposts
avgpos = positive / allposts

print("Avg positive = ", avgpos)
print("Avg negative = ", avgneg)
classifier.show_most_informative_features()

#Removes Hours, Minutes and seconds from dataframe
dataTime['Date'] = pd.to_datetime(dataTime['Date']).dt.date
#Groups entries by date and calculates the mean value of the sentiment scores
avgScore = dataTime.groupby('Date')['Sentiment'].mean()
    
print("The Average sentiment = " + avgScore)
